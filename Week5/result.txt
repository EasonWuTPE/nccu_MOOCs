The total of data x_train  is 60000, size is 28 x 28.
The total of data x_test is 10000, size is 28 x 28.
Total training sample is 60000, (12665, 784) of which is 0 and 1.
Total testing sample is 10000, (2115, 784) of which is 0 and 1.
So the sample's distributions is uniform that.

 Summary of model_0_to_9: 
 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 500)               392500    
_________________________________________________________________
activation_1 (Activation)    (None, 500)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 500)               250500    
_________________________________________________________________
activation_2 (Activation)    (None, 500)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                5010      
_________________________________________________________________
activation_3 (Activation)    (None, 10)                0         
=================================================================
Total params: 648,010
Trainable params: 648,010
Non-trainable params: 0
_________________________________________________________________

 Summary of model_0_to_1: 
 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 500)               392500    
_________________________________________________________________
activation_1 (Activation)    (None, 500)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 500)               250500    
_________________________________________________________________
activation_2 (Activation)    (None, 500)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1002      
_________________________________________________________________
activation_4 (Activation)    (None, 2)                 0         
=================================================================
Total params: 644,002
Trainable params: 644,002
Non-trainable params: 0
_________________________________________________________________
>> We can find that except the output layer, the name of the other hidden layers are the same.

But the "Trainable params" is 644002 and "Total params" is 644002. Means that the whole model are trained again. We want to use the same params of the hiiden layers to train the new model.
 
Use the attribute trainable = False to not to re-train the model.
 
The model summary is: 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 500)               392500    
_________________________________________________________________
activation_1 (Activation)    (None, 500)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 500)               250500    
_________________________________________________________________
activation_2 (Activation)    (None, 500)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1002      
_________________________________________________________________
activation_4 (Activation)    (None, 2)                 0         
=================================================================
Total params: 644,002
Trainable params: 1,002
Non-trainable params: 643,000
_________________________________________________________________
The trainable params reduce from 644002 to 1002 which is params of the output layer.

Epoch 1/5

  100/12665 [..............................] - ETA: 19s - loss: 0.3401 - acc: 0.4400
  600/12665 [>.............................] - ETA: 4s - loss: 0.2702 - acc: 0.5383 
  900/12665 [=>............................] - ETA: 3s - loss: 0.2484 - acc: 0.6467
 1200/12665 [=>............................] - ETA: 3s - loss: 0.2307 - acc: 0.7217
 1600/12665 [==>...........................] - ETA: 2s - loss: 0.2120 - acc: 0.7825
 2100/12665 [===>..........................] - ETA: 2s - loss: 0.1930 - acc: 0.8305
 2600/12665 [=====>........................] - ETA: 2s - loss: 0.1773 - acc: 0.8588
 3100/12665 [======>.......................] - ETA: 1s - loss: 0.1647 - acc: 0.8800
 3700/12665 [=======>......................] - ETA: 1s - loss: 0.1515 - acc: 0.8984
 4300/12665 [=========>....................] - ETA: 1s - loss: 0.1404 - acc: 0.9121
 4800/12665 [==========>...................] - ETA: 1s - loss: 0.1326 - acc: 0.9208
 5200/12665 [===========>..................] - ETA: 1s - loss: 0.1268 - acc: 0.9262
 5400/12665 [===========>..................] - ETA: 1s - loss: 0.1243 - acc: 0.9287
 5600/12665 [============>.................] - ETA: 1s - loss: 0.1216 - acc: 0.9313
 5800/12665 [============>.................] - ETA: 1s - loss: 0.1190 - acc: 0.9336
 6300/12665 [=============>................] - ETA: 1s - loss: 0.1134 - acc: 0.9386
 6700/12665 [==============>...............] - ETA: 1s - loss: 0.1093 - acc: 0.9421
 7100/12665 [===============>..............] - ETA: 0s - loss: 0.1056 - acc: 0.9452
 7400/12665 [================>.............] - ETA: 0s - loss: 0.1029 - acc: 0.9473
 7700/12665 [=================>............] - ETA: 0s - loss: 0.1003 - acc: 0.9494
 8100/12665 [==================>...........] - ETA: 0s - loss: 0.0971 - acc: 0.9517
 8500/12665 [===================>..........] - ETA: 0s - loss: 0.0942 - acc: 0.9538
 8700/12665 [===================>..........] - ETA: 0s - loss: 0.0927 - acc: 0.9547
 8900/12665 [====================>.........] - ETA: 0s - loss: 0.0914 - acc: 0.9556
 9200/12665 [====================>.........] - ETA: 0s - loss: 0.0893 - acc: 0.9571
 9700/12665 [=====================>........] - ETA: 0s - loss: 0.0862 - acc: 0.9591
10100/12665 [======================>.......] - ETA: 0s - loss: 0.0840 - acc: 0.9605
10600/12665 [========================>.....] - ETA: 0s - loss: 0.0814 - acc: 0.9624
11000/12665 [=========================>....] - ETA: 0s - loss: 0.0793 - acc: 0.9637
11400/12665 [==========================>...] - ETA: 0s - loss: 0.0776 - acc: 0.9647
11800/12665 [==========================>...] - ETA: 0s - loss: 0.0757 - acc: 0.9658
12200/12665 [===========================>..] - ETA: 0s - loss: 0.0741 - acc: 0.9667
12500/12665 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9674
12665/12665 [==============================] - 2s 169us/step - loss: 0.0722 - acc: 0.9678
Epoch 2/5

  100/12665 [..............................] - ETA: 4s - loss: 0.0225 - acc: 0.9900
  500/12665 [>.............................] - ETA: 2s - loss: 0.0222 - acc: 0.9960
  900/12665 [=>............................] - ETA: 1s - loss: 0.0208 - acc: 0.9978
 1300/12665 [==>...........................] - ETA: 1s - loss: 0.0212 - acc: 0.9977
 1800/12665 [===>..........................] - ETA: 1s - loss: 0.0212 - acc: 0.9978
 2300/12665 [====>.........................] - ETA: 1s - loss: 0.0208 - acc: 0.9974
 2800/12665 [=====>........................] - ETA: 1s - loss: 0.0205 - acc: 0.9975
 3300/12665 [======>.......................] - ETA: 1s - loss: 0.0201 - acc: 0.9976
 3700/12665 [=======>......................] - ETA: 1s - loss: 0.0197 - acc: 0.9978
 4000/12665 [========>.....................] - ETA: 1s - loss: 0.0195 - acc: 0.9978
 4200/12665 [========>.....................] - ETA: 1s - loss: 0.0195 - acc: 0.9974
 4600/12665 [=========>....................] - ETA: 1s - loss: 0.0193 - acc: 0.9976
 5100/12665 [===========>..................] - ETA: 1s - loss: 0.0191 - acc: 0.9976
 5400/12665 [===========>..................] - ETA: 1s - loss: 0.0191 - acc: 0.9974
 5800/12665 [============>.................] - ETA: 0s - loss: 0.0188 - acc: 0.9976
 6200/12665 [=============>................] - ETA: 0s - loss: 0.0187 - acc: 0.9974
 6600/12665 [==============>...............] - ETA: 0s - loss: 0.0185 - acc: 0.9974
 7000/12665 [===============>..............] - ETA: 0s - loss: 0.0184 - acc: 0.9971
 7500/12665 [================>.............] - ETA: 0s - loss: 0.0181 - acc: 0.9972
 7700/12665 [=================>............] - ETA: 0s - loss: 0.0179 - acc: 0.9973
 8000/12665 [=================>............] - ETA: 0s - loss: 0.0178 - acc: 0.9973
 8200/12665 [==================>...........] - ETA: 0s - loss: 0.0177 - acc: 0.9972
 8700/12665 [===================>..........] - ETA: 0s - loss: 0.0176 - acc: 0.9971
 9200/12665 [====================>.........] - ETA: 0s - loss: 0.0175 - acc: 0.9971
 9700/12665 [=====================>........] - ETA: 0s - loss: 0.0172 - acc: 0.9971
10200/12665 [=======================>......] - ETA: 0s - loss: 0.0169 - acc: 0.9971
10700/12665 [========================>.....] - ETA: 0s - loss: 0.0168 - acc: 0.9970
11200/12665 [=========================>....] - ETA: 0s - loss: 0.0167 - acc: 0.9970
11600/12665 [==========================>...] - ETA: 0s - loss: 0.0165 - acc: 0.9971
12000/12665 [===========================>..] - ETA: 0s - loss: 0.0164 - acc: 0.9969
12200/12665 [===========================>..] - ETA: 0s - loss: 0.0163 - acc: 0.9970
12400/12665 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9970
12665/12665 [==============================] - 2s 145us/step - loss: 0.0162 - acc: 0.9971
Epoch 3/5

  100/12665 [..............................] - ETA: 2s - loss: 0.0114 - acc: 1.0000
  500/12665 [>.............................] - ETA: 1s - loss: 0.0114 - acc: 0.9980
 1000/12665 [=>............................] - ETA: 1s - loss: 0.0111 - acc: 0.9970
 1500/12665 [==>...........................] - ETA: 1s - loss: 0.0118 - acc: 0.9967
 2000/12665 [===>..........................] - ETA: 1s - loss: 0.0115 - acc: 0.9970
 2500/12665 [====>.........................] - ETA: 1s - loss: 0.0113 - acc: 0.9972
 3000/12665 [======>.......................] - ETA: 1s - loss: 0.0112 - acc: 0.9977
 3500/12665 [=======>......................] - ETA: 1s - loss: 0.0111 - acc: 0.9977
 3800/12665 [========>.....................] - ETA: 1s - loss: 0.0110 - acc: 0.9979
 4100/12665 [========>.....................] - ETA: 1s - loss: 0.0110 - acc: 0.9978
 4400/12665 [=========>....................] - ETA: 1s - loss: 0.0110 - acc: 0.9977
 5000/12665 [==========>...................] - ETA: 0s - loss: 0.0108 - acc: 0.9980
 5600/12665 [============>.................] - ETA: 0s - loss: 0.0108 - acc: 0.9977
 6200/12665 [=============>................] - ETA: 0s - loss: 0.0109 - acc: 0.9973
 6800/12665 [===============>..............] - ETA: 0s - loss: 0.0108 - acc: 0.9974
 7400/12665 [================>.............] - ETA: 0s - loss: 0.0108 - acc: 0.9969
 7900/12665 [=================>............] - ETA: 0s - loss: 0.0107 - acc: 0.9971
 8500/12665 [===================>..........] - ETA: 0s - loss: 0.0105 - acc: 0.9973
 9000/12665 [====================>.........] - ETA: 0s - loss: 0.0104 - acc: 0.9974
 9200/12665 [====================>.........] - ETA: 0s - loss: 0.0104 - acc: 0.9974
 9400/12665 [=====================>........] - ETA: 0s - loss: 0.0104 - acc: 0.9973
 9700/12665 [=====================>........] - ETA: 0s - loss: 0.0104 - acc: 0.9974
10200/12665 [=======================>......] - ETA: 0s - loss: 0.0104 - acc: 0.9973
10700/12665 [========================>.....] - ETA: 0s - loss: 0.0103 - acc: 0.9973
11200/12665 [=========================>....] - ETA: 0s - loss: 0.0104 - acc: 0.9970
11700/12665 [==========================>...] - ETA: 0s - loss: 0.0103 - acc: 0.9969
12300/12665 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9971
12665/12665 [==============================] - 2s 121us/step - loss: 0.0101 - acc: 0.9972
Epoch 4/5

  100/12665 [..............................] - ETA: 2s - loss: 0.0146 - acc: 0.9900
  700/12665 [>.............................] - ETA: 1s - loss: 0.0093 - acc: 0.9971
 1300/12665 [==>...........................] - ETA: 1s - loss: 0.0098 - acc: 0.9962
 1900/12665 [===>..........................] - ETA: 1s - loss: 0.0091 - acc: 0.9974
 2500/12665 [====>.........................] - ETA: 0s - loss: 0.0086 - acc: 0.9972
 3100/12665 [======>.......................] - ETA: 0s - loss: 0.0085 - acc: 0.9971
 3700/12665 [=======>......................] - ETA: 0s - loss: 0.0085 - acc: 0.9968
 4200/12665 [========>.....................] - ETA: 0s - loss: 0.0084 - acc: 0.9971
 4800/12665 [==========>...................] - ETA: 0s - loss: 0.0085 - acc: 0.9971
 5400/12665 [===========>..................] - ETA: 0s - loss: 0.0084 - acc: 0.9969
 6000/12665 [=============>................] - ETA: 0s - loss: 0.0083 - acc: 0.9972
 6600/12665 [==============>...............] - ETA: 0s - loss: 0.0081 - acc: 0.9973
 7200/12665 [================>.............] - ETA: 0s - loss: 0.0081 - acc: 0.9972
 7800/12665 [=================>............] - ETA: 0s - loss: 0.0080 - acc: 0.9973
 8400/12665 [==================>...........] - ETA: 0s - loss: 0.0079 - acc: 0.9974
 9000/12665 [====================>.........] - ETA: 0s - loss: 0.0078 - acc: 0.9973
 9600/12665 [=====================>........] - ETA: 0s - loss: 0.0078 - acc: 0.9974
10200/12665 [=======================>......] - ETA: 0s - loss: 0.0078 - acc: 0.9974
10800/12665 [========================>.....] - ETA: 0s - loss: 0.0077 - acc: 0.9975
11400/12665 [==========================>...] - ETA: 0s - loss: 0.0077 - acc: 0.9974
12000/12665 [===========================>..] - ETA: 0s - loss: 0.0078 - acc: 0.9973
12600/12665 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9972
12665/12665 [==============================] - 1s 95us/step - loss: 0.0078 - acc: 0.9972
Epoch 5/5

  100/12665 [..............................] - ETA: 1s - loss: 0.0054 - acc: 1.0000
  700/12665 [>.............................] - ETA: 1s - loss: 0.0056 - acc: 1.0000
 1300/12665 [==>...........................] - ETA: 1s - loss: 0.0061 - acc: 0.9992
 1800/12665 [===>..........................] - ETA: 1s - loss: 0.0068 - acc: 0.9978
 2400/12665 [====>.........................] - ETA: 1s - loss: 0.0072 - acc: 0.9971
 3000/12665 [======>.......................] - ETA: 0s - loss: 0.0072 - acc: 0.9973
 3600/12665 [=======>......................] - ETA: 0s - loss: 0.0069 - acc: 0.9978
 4200/12665 [========>.....................] - ETA: 0s - loss: 0.0070 - acc: 0.9976
 4800/12665 [==========>...................] - ETA: 0s - loss: 0.0072 - acc: 0.9971
 5400/12665 [===========>..................] - ETA: 0s - loss: 0.0071 - acc: 0.9972
 6000/12665 [=============>................] - ETA: 0s - loss: 0.0071 - acc: 0.9968
 6600/12665 [==============>...............] - ETA: 0s - loss: 0.0071 - acc: 0.9967
 7200/12665 [================>.............] - ETA: 0s - loss: 0.0071 - acc: 0.9965
 7800/12665 [=================>............] - ETA: 0s - loss: 0.0070 - acc: 0.9968
 8400/12665 [==================>...........] - ETA: 0s - loss: 0.0068 - acc: 0.9969
 9000/12665 [====================>.........] - ETA: 0s - loss: 0.0067 - acc: 0.9969
 9600/12665 [=====================>........] - ETA: 0s - loss: 0.0068 - acc: 0.9968
10200/12665 [=======================>......] - ETA: 0s - loss: 0.0067 - acc: 0.9969
10800/12665 [========================>.....] - ETA: 0s - loss: 0.0066 - acc: 0.9969
11400/12665 [==========================>...] - ETA: 0s - loss: 0.0066 - acc: 0.9970
12000/12665 [===========================>..] - ETA: 0s - loss: 0.0066 - acc: 0.9970
12500/12665 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9971
12665/12665 [==============================] - 1s 96us/step - loss: 0.0065 - acc: 0.9971

  32/2115 [..............................] - ETA: 2s
 512/2115 [======>.......................] - ETA: 0s
 992/2115 [=============>................] - ETA: 0s
1472/2115 [===================>..........] - ETA: 0s
1952/2115 [==========================>...] - ETA: 0s
2115/2115 [==============================] - 0s 124us/step
The Loss is 0.004853, the accuracy is 0.998582.
